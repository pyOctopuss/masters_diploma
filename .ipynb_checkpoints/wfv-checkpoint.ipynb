{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc5c74dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))\n",
    "\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 12,8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d57b9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import re\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "import yaml\n",
    "from yaml import dump\n",
    "import uuid\n",
    "import itertools\n",
    "from shutil import copy2\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "194a5fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "from prophet import Prophet\n",
    "\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6627bb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2392ee5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908c4f54",
   "metadata": {},
   "source": [
    "## Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5172d8a5",
   "metadata": {},
   "source": [
    "#### initialize all required valiables, prepare datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "042f2e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init():\n",
    "\n",
    "    train_end = datetime(2025, 8, 31, 23)\n",
    "    test_start = datetime(2025, 9, 1, 0)\n",
    "    test_end = datetime(2025, 9, 14, 23)\n",
    "    \n",
    "    train_features_set = [\n",
    "        ['month', 'week_day', 'year_day', 'is_day', 'daylight_seconds', 'temperature_mean_3_weeks', 'temperature_mean_3_years'],\n",
    "#         ['month', 'year_day', 'is_day', 'sunshine_duration', 'temperature_min_3_weeks', 'temperature_max_3_weeks', 'temperature_mean_3_weeks',\n",
    "#          'temperature_min_3_years', 'temperature_max_3_years', 'temperature_mean_3_years', 'cloud_cover_mean_7_days', 'pressure_msl_mean_7_days'],\n",
    "#         ['month', 'year_day', 'is_day', 'sunshine_duration', 'temperature_min_3_weeks', 'temperature_max_3_weeks', 'temperature_mean_3_weeks',\n",
    "#         'relative_humidity_min_7_days', 'temperature_min_7_days', 'temperature_mean_7_days', 'temperature_lag_168'],\n",
    "        ['month', 'year_day', 'is_day', 'sunshine_duration', 'temperature_min_3_weeks', 'temperature_max_3_weeks', 'temperature_mean_3_weeks']\n",
    "    ]\n",
    "\n",
    "    \n",
    "    date_parse = lambda dates: pd.to_datetime(dates)\n",
    "    path = f\"/masters_diploma/processed_data/history_weather_with_daylight.csv\"\n",
    "    \n",
    "    full_set = pd.read_csv(\n",
    "        path,\n",
    "        parse_dates=[\"date\"],\n",
    "        date_parser=date_parse,\n",
    "        index_col=[\"date\"],\n",
    "    )\n",
    "    \n",
    "\n",
    "    full_set = full_set[:test_end].fillna(0)\n",
    "#     full_set = future_target(full_set, test_start, test_end)\n",
    "    \n",
    "    \n",
    "#     test_start = datetime(full_set.loc[test_start:].index[0].year, full_set.loc[test_start:].index[0].month, full_set.loc[test_start:].index[0].day)\n",
    "#     train_end = datetime(full_set.loc[:train_end].index[-1].year, full_set.loc[:train_end].index[-1].month, full_set.loc[:train_end].index[-1].day)\n",
    "    \n",
    "    \n",
    "    return full_set, train_end, test_start, test_end, train_features_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e12930c",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def future_target(df, date_start, date_end):\n",
    "    \n",
    "    date_range = pd.date_range(date_start, date_end, freq='H')\n",
    "    \n",
    "    add_df = pd.DataFrame(index=date_range, columns=df.columns)\n",
    "    \n",
    "#     add_df\n",
    "    \n",
    "    df = pd.concat([df, add_df])\n",
    "    \n",
    "    return df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c42b5f2c",
   "metadata": {
    "code_folding": [
     0,
     18,
     43,
     66
    ]
   },
   "outputs": [],
   "source": [
    "def models_hyperparameter_random_forest():\n",
    "\n",
    "    depth_list = [4, 5, 6, 7]\n",
    "    n_estimators_list = [50, 100, 200, 500, 1000]\n",
    "    \n",
    "    hyperparameters_for_model = []\n",
    "    \n",
    "    for depth, n_estimators in itertools.product(depth_list, n_estimators_list):\n",
    "        hyperparameters_for_model.append({\n",
    "                        'n_estimators': n_estimators,\n",
    "                        'n_jobs': -1,\n",
    "                        'random_state': 2,\n",
    "                        'max_depth': depth,\n",
    "            })\n",
    "\n",
    "    return hyperparameters_for_model\n",
    "\n",
    "\n",
    "def models_hyperparameter_xgboost():\n",
    "\n",
    "    depth_list = [5, 7, 9]\n",
    "    n_estimators_list = [50, 100, 200, 500, 1000]\n",
    "    \n",
    "    hyperparameters_for_model = []\n",
    "    \n",
    "    for depth, n_estimators in itertools.product(depth_list, n_estimators_list):\n",
    "        hyperparameters_for_model.append({\n",
    "                    'n_estimators': n_estimators,\n",
    "                    'n_jobs': -1,\n",
    "                    'max_depth': depth,\n",
    "                    'eta': 0.3,\n",
    "                    'booster': 'gbtree',\n",
    "                    'objective': 'reg:squarederror',\n",
    "                    'eval_metric': 'rmse',\n",
    "                    'subsample': 1,\n",
    "                    'colsample_bytree': 1,\n",
    "                    'min_child_weight': 1,\n",
    "                    'random_state': 2,\n",
    "            })\n",
    "\n",
    "    return hyperparameters_for_model\n",
    "\n",
    "\n",
    "def models_hyperparameter_lgbm():\n",
    "\n",
    "    \n",
    "    depth_list = [6, 7]\n",
    "    n_estimators_list = [10, 50, 100]\n",
    "    \n",
    "    hyperparameters_for_model = []\n",
    "    \n",
    "    for depth, n_estimators in itertools.product(depth_list, n_estimators_list):\n",
    "        hyperparameters_for_model.append({\n",
    "                    'n_estimators': n_estimators,\n",
    "                    'n_jobs': -1,\n",
    "                    'max_depth': depth,\n",
    "                    'eta': 0.3,\n",
    "                    'random_state': 2,\n",
    "                    'objective': 'binary',\n",
    "                    'verbosity': -1,\n",
    "                    'metric': 'binary', \n",
    "            })\n",
    "\n",
    "    return hyperparameters_for_model\n",
    "\n",
    "\n",
    "def models_hyperparameter_prophet():\n",
    "    \n",
    "    hyperparameters_for_model.append({})\n",
    "\n",
    "    seasonality_list = [\"seasonality_yearly\", \"seasonality_daily\"]\n",
    "    season_list = [\"additive\", \"multiplicative\"]\n",
    "    \n",
    "    hyperparameters_for_model = []\n",
    "    \n",
    "    for seasonality, season in itertools.product(seasonality_list, season_list):\n",
    "        hyperparameters_for_model.append({\n",
    "                        'growth': \"logistic\",\n",
    "                        'season': season,\n",
    "                        f'{seasonality}': True,\n",
    "            })\n",
    "\n",
    "    return hyperparameters_for_model\n",
    "\n",
    "\n",
    "def models_hyperparameter_sarimax():\n",
    "\n",
    "    s = 24\n",
    "\n",
    "    p_list, d_list, q_list = [1], [1], [1]\n",
    "    P_list, D_list, Q_list = [1], [0, 1], [1]\n",
    "    trend_list = [\"n\", \"c\"]\n",
    "\n",
    "    hyperparameters_for_model = []\n",
    "    \n",
    "    for (p, d, q, P, D, Q, trend) in itertools.product(p_list, d_list, q_list, P_list, D_list, Q_list, trend_list):\n",
    "        if d == 0 and D == 0 and trend == \"n\":\n",
    "            continue\n",
    "\n",
    "        hp = dict(\n",
    "            order=(p, d, q),\n",
    "            seasonal_order=(P, D, Q, s),\n",
    "            trend=trend,\n",
    "            enforce_stationarity=False,\n",
    "            enforce_invertibility=False,\n",
    "            concentrate_scale=True,\n",
    "        )\n",
    "        hyperparameters_for_model.append(hp)\n",
    "\n",
    "    return hyperparameters_for_model\n",
    "\n",
    "\n",
    "def fit_options_sarimax_simple():\n",
    "    return [\n",
    "        dict(disp=False),\n",
    "        dict(method=\"lbfgs\", maxiter=200, disp=False, cov_type=\"opg\"),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01393351",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def define_parameters(train_end, test_start, test_end, train_features_set, forecast_steps, models_dict):\n",
    "    \n",
    "    list_of_configs = []\n",
    "    \n",
    "    model = None\n",
    "    \n",
    "#     for duration in [90, 240]:\n",
    "    for train_start in [datetime(2024, 9, 1), datetime(2024, 1, 1), datetime(2023, 9, 1)]:\n",
    "        for md in models_dict.values():\n",
    "            if md == 'random_forest':\n",
    "                hyperparameters_for_model = models_hyperparameter_random_forest()\n",
    "            elif md == 'xgboost':\n",
    "                hyperparameters_for_model = models_hyperparameter_xgboost()\n",
    "            elif md == 'lightgbm':\n",
    "                hyperparameters_for_model = models_hyperparameter_lgbm()\n",
    "            elif md == 'prophet':\n",
    "                hyperparameters_for_model = models_hyperparameter_prophet()\n",
    "            elif md == 'sarimax':\n",
    "                hyperparameters_for_model = models_hyperparameter_sarimax()\n",
    "            else:\n",
    "                print('Unknown model')\n",
    "                return\n",
    "\n",
    "\n",
    "            for hp in hyperparameters_for_model:\n",
    "\n",
    "                if md == 'random_forest':\n",
    "                    model = RandomForestRegressor(**hp)\n",
    "                    fit_kwargs_list = [None]\n",
    "                elif md == 'xgboost':\n",
    "                    model = XGBRegressor(**hp)\n",
    "                    fit_kwargs_list = [None]\n",
    "                elif md == 'lightgbm':\n",
    "                    model = LGBMRegressor(**hp)\n",
    "                    fit_kwargs_list = [None]\n",
    "                elif md == 'prophet':\n",
    "                    model = Prophet()\n",
    "    #                 model = Prophet(**hp)\n",
    "                    fit_kwargs_list = [None]\n",
    "                elif md == 'sarimax':\n",
    "                    model = partial(SARIMAX, **hp)\n",
    "                    fit_kwargs_list = fit_options_sarimax_simple()\n",
    "                else:\n",
    "                    print('Unknown model')\n",
    "                    \n",
    "                    \n",
    "                for kw in fit_kwargs_list:\n",
    "                    \n",
    "                    for train_features in train_features_set:\n",
    "                        config = {\n",
    "                            'unique_uuid': str(uuid.uuid1()),\n",
    "                            'train_start': train_start,\n",
    "                            'train_end': train_end,\n",
    "                            'test_start': test_start,\n",
    "                            'test_end': test_end,\n",
    "        #                     'duration_training_history': duration,\n",
    "                            'target_column': 'temperature',\n",
    "                            'train_features': train_features,\n",
    "                            'path_to_result': f'/masters_diploma/',\n",
    "                            'forecast_days': forecast_steps,\n",
    "            #                 'hour_mean_value': 5,\n",
    "                            'model_name': md,\n",
    "                            'model': model,\n",
    "                            'model_hyperparameters': hp,\n",
    "                        }\n",
    "                        if md == 'sarimax':\n",
    "                            config['fit_kwargs'] = kw\n",
    "\n",
    "                        list_of_configs.append(config.copy())\n",
    "    \n",
    "    return list_of_configs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de80c2e",
   "metadata": {},
   "source": [
    "#### functions used in wfv service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4016f075",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def data(day, X_full_set, y_full_set, train_start, config, forecast_steps):\n",
    "\n",
    "    X_train = X_full_set.loc[train_start:config[\"train_end\"]]\n",
    "    X_test = X_full_set.loc[config[\"test_start\"]+timedelta(days=day): config[\"test_start\"]+timedelta(days=day+forecast_steps, hours=23)]\n",
    "    \n",
    "    y_train = y_full_set.loc[train_start:config[\"train_end\"]]\n",
    "    y_test = y_full_set.loc[config[\"test_start\"]+timedelta(days=day): config[\"test_start\"]+timedelta(days=day+forecast_steps, hours=23)]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e1c8fa3",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def standardize_mean_values(day, df_test, df_train, full_set, config):\n",
    "    \n",
    "    agg_cols = [col for col in config['train_features'] if (col.startswith(f\"{config['target_column']}_m\")) & (col.endswith('days'))]\n",
    "\n",
    "    if agg_cols:\n",
    "        for agg in agg_cols:\n",
    "            if agg in df_test.columns:\n",
    "                try:\n",
    "#                 print(config[\"test_start\"]+timedelta(days=day), df_test.loc[config[\"test_start\"]+timedelta(days=day), agg])\n",
    "                    num = df_test.loc[config[\"test_start\"]+timedelta(days=day), agg]\n",
    "\n",
    "                except KeyError as e:\n",
    "                    num = df_train[agg].iloc[-1]\n",
    "\n",
    "                finally:\n",
    "\n",
    "                    _df = df_test.loc[config[\"test_start\"]+timedelta(days=day):, agg]\n",
    "                    _df = _df.replace(_df.values, num)\n",
    "\n",
    "#                 print(df_test.loc[config[\"test_start\"]+timedelta(days=day):, agg], _df.values.ravel())\n",
    "\n",
    "                    df_test.loc[config[\"test_start\"]:, agg] = _df.values.ravel()\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    \n",
    "    return df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf71e08e",
   "metadata": {
    "code_folding": [
     18,
     23
    ]
   },
   "outputs": [],
   "source": [
    "def estimations(day, df_stats, y_pred_df, y_test, config):\n",
    "    \n",
    "    dates = y_test.index\n",
    "    \n",
    "    for date in dates:\n",
    "        step_day = int((date-(config[\"test_start\"]+timedelta(days=day))).days)\n",
    "\n",
    "        try:\n",
    "            pred = y_pred_df.loc[date, 'predicted_mean']\n",
    "            real = y_test.loc[date, config['target_column']]\n",
    "\n",
    "            err = abs(pred / real - 1) * 100\n",
    "\n",
    "            df_stats.loc[date, f'd-{step_day}' + '_total_abs_error'] = np.round(abs(pred-real))\n",
    "            df_stats.loc[date, f'd-{step_day}' + '_total_relative_error'] = np.round(abs(pred / real - 1), 4) * 100\n",
    "            df_stats.loc[date, f'd-{step_day}' + '_more_5'] = 1 if (err > 5) else 0\n",
    "            df_stats.loc[date, f'd-{step_day}' + '_more_10'] = 1 if (err > 10) else 0\n",
    "            \n",
    "        except ZeroDivisionError as e:\n",
    "            print(e)\n",
    "\n",
    "            df_stats.loc[date, :] = 0\n",
    "\n",
    "        except KeyError as e:\n",
    "            print(e)\n",
    "\n",
    "            df_stats.loc[date, :] = 0\n",
    "    \n",
    "    return df_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "664c8058",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_predictions(day, forecast_steps, y_pred_df, config, research_task_uuid):\n",
    "    \n",
    "    for step in range(forecast_steps+1):\n",
    "        try:\n",
    "            pred = y_pred_df.iloc[24*step:24*(step+1), 0].dropna().sort_index()\n",
    "            pred.index.name = 'date_time'\n",
    "\n",
    "            path_to_files = os.path.join(config['path_to_result'], \"forecast\", config['model_name'], \n",
    "                                         f\"research_task_{research_task_uuid}\", \n",
    "                                         f\"{config['model_name']}_{config['unique_uuid']}\")\n",
    "            if not os.path.isdir(path_to_files):\n",
    "                os.makedirs(path_to_files)\n",
    "                \n",
    "            file_name = os.path.join(path_to_files, \n",
    "                    f\"forecast_d-{step}_{config['model_name']}_{(config['test_start']+timedelta(days=day)).strftime('%Y-%m-%d')}_({pred.index[0].strftime('%Y-%m-%d')}).csv\")\n",
    "\n",
    "            pd.DataFrame(pred).to_csv(file_name)\n",
    "\n",
    "        except KeyError:\n",
    "            pass\n",
    "        \n",
    "        except IndexError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41fc750",
   "metadata": {},
   "source": [
    "### wfv service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c32598ef",
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_wfv(full_set: pd.DataFrame, config: dict, research_task_uuid: str, forecast_steps: int, models_dict: dict):\n",
    "    \n",
    "    X_full_set = full_set.loc[:, config['train_features']]\n",
    "    y_full_set = full_set.loc[:, [config['target_column']]]\n",
    "    \n",
    "    if X_full_set.shape[0] != y_full_set.shape[0]:\n",
    "        common_index = list(set(X_full_set.index) & set(y_full_set.index))\n",
    "        common_index.sort()\n",
    "        X_full_set = X_full_set.loc[common_index, :]\n",
    "        y_full_set = y_full_set.loc[common_index, :]\n",
    "    print(X_full_set.shape, y_full_set.shape)\n",
    "    \n",
    "\n",
    "    df_preds = pd.DataFrame()\n",
    "    df_stats = pd.DataFrame()\n",
    "\n",
    "    count_days = (test_end - test_start).days + 1\n",
    "    \n",
    "    \n",
    "    model_name = config['model_name']\n",
    "    print(model_name)\n",
    "    \n",
    "    model = config['model']\n",
    "    kwargs = config['fit_kwargs']\n",
    "\n",
    "    unique_uuid = config['unique_uuid']\n",
    "    \n",
    "    if not os.path.isdir(config['path_to_result']):\n",
    "        os.makedirs(config['path_to_result'])\n",
    "\n",
    "    path_folder_result = os.path.join(config['path_to_result'], \"wf_result\", model_name,\n",
    "                                      f\"research_task_{research_task_uuid}\")\n",
    "    if not os.path.isdir(path_folder_result):\n",
    "        os.makedirs(path_folder_result)\n",
    "        \n",
    "        \n",
    "\n",
    "    for day in tqdm(range(count_days)):\n",
    "        \n",
    "        \n",
    "        train_start = config.get('train_start', None)\n",
    "        if train_start is None:\n",
    "            if config.get('duration_training_history', None) is None:\n",
    "                train_start = X_full_set.index[0]\n",
    "                config['train_start'] = datetime(train_start.year, train_start.month, train_start.day)\n",
    "            else:\n",
    "                train_start = config['train_end'] + timedelta(days=i - config['duration_training_history'])\n",
    "\n",
    "        try:\n",
    "\n",
    "            X_train, X_test, y_train, y_test = data(day, X_full_set, y_full_set, train_start, config, forecast_steps)\n",
    "            X_test = standardize_mean_values(day, X_test.copy(), X_train, full_set, config)\n",
    "\n",
    "            if model_name == 'sarimax':\n",
    "                y_pred = model(endog=y_train, exog=X_train).fit(**kwargs).get_forecast(steps=len(y_test), exog=X_test).predicted_mean\n",
    "            else:\n",
    "                y_pred = model.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "\n",
    "            y_pred_df = pd.DataFrame(y_pred, index=y_test.index)\n",
    "            y_pred_df.columns = ['predicted_mean']\n",
    "            \n",
    "            write_predictions(day, forecast_steps, y_pred_df, config, research_task_uuid)\n",
    "\n",
    "            df_stats = estimations(day, df_stats, y_pred_df, y_test, config)\n",
    "#             print('\\n\\n')\n",
    "        \n",
    "\n",
    "\n",
    "        except KeyError as e:\n",
    "            print(e)\n",
    "            continue\n",
    "            \n",
    "        except ValueError as e:\n",
    "            print(e)\n",
    "            continue\n",
    "\n",
    "\n",
    "    last_index = df_stats.index[-1]\n",
    "    df_stats.loc[last_index, 'model_hyperparameters'] = str(config['model_hyperparameters'])\n",
    "    df_stats.loc[last_index, 'train_features'] = str(config['train_features'])\n",
    "    \n",
    "    path_to_save_result_csv = os.path.join(path_folder_result, f'{model_name}_{unique_uuid}.csv')\n",
    "    df_stats.round(2).to_csv(path_to_save_result_csv, date_format='%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    config_to_save = config.copy()\n",
    "    config_to_save.pop('model', None)\n",
    "    with open(os.path.join(path_folder_result, f'{model_name}_{unique_uuid}.yaml'), 'w') as outfile:\n",
    "        dump(config_to_save, outfile, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0650674",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train ends: 2025-08-31 23:00:00\t test starts: 2025-09-01 00:00:00\n",
      "_research_task_uuid = f04c327d-b822-11f0-aad6-b1b3fada54e9\n",
      "\n",
      "count_configs 48 \n",
      "\n",
      "0 sarimax == {'unique_uuid': 'f04c327e-b822-11f0-be2f-b1b3fada54e9', 'train_start': datetime.datetime(2024, 9, 1, 0, 0), 'train_end': datetime.datetime(2025, 8, 31, 23, 0), 'test_start': datetime.datetime(2025, 9, 1, 0, 0), 'test_end': datetime.datetime(2025, 9, 14, 23, 0), 'target_column': 'temperature', 'train_features': ['month', 'week_day', 'year_day', 'is_day', 'daylight_seconds', 'temperature_mean_3_weeks', 'temperature_mean_3_years'], 'path_to_result': '/masters_diploma/', 'forecast_days': 3, 'model_name': 'sarimax', 'model': functools.partial(<class 'statsmodels.tsa.statespace.sarimax.SARIMAX'>, order=(1, 1, 1), seasonal_order=(1, 0, 1, 24), trend='n', enforce_stationarity=False, enforce_invertibility=False, concentrate_scale=True), 'model_hyperparameters': {'order': (1, 1, 1), 'seasonal_order': (1, 0, 1, 24), 'trend': 'n', 'enforce_stationarity': False, 'enforce_invertibility': False, 'concentrate_scale': True}, 'fit_kwargs': {'disp': False}} \n",
      "\n",
      "(93840, 7) (93840, 1)\n",
      "sarimax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/14 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "models_list = ['SARIMAX']    #'Prophet', 'XGBoost', 'LightGBM', 'Random_Forest'\n",
    "models_dict = dict([(\"\".join(re.findall('([A-Z])', k)).lower(), k.lower()) for k in models_list])\n",
    "\n",
    "forecast_steps = 3        # means that forecast will be made on {n} futute days \n",
    "\n",
    "full_set, train_end, test_start, test_end, train_features_set = init()\n",
    "print(f'train ends: {train_end}\\t test starts: {test_start}')\n",
    "\n",
    "_research_task_uuid = str(uuid.uuid1())\n",
    "print(f'_research_task_uuid = {_research_task_uuid}\\n')\n",
    "\n",
    "configs = define_parameters(train_end, test_start, test_end, train_features_set, forecast_steps, models_dict)\n",
    "print(f'count_configs {len(configs)} \\n')\n",
    "\n",
    "for i, _ in enumerate(configs):\n",
    "    print(i, _['model_name'], '==', _, '\\n')\n",
    "\n",
    "    run_wfv(full_set, _, _research_task_uuid, forecast_steps, models_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832db5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "copy2('wfv.ipynb', f'/masters_diploma/archive/wfv_{str(uuid.uuid1())}_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
