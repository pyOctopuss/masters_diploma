{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c71220b6-2359-42b1-bc2f-83dd44049cf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "406b1fd0-b5b5-4083-af14-14f04d5192bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import re\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "import yaml\n",
    "from yaml import dump\n",
    "import uuid\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7437fae-d5e5-4c67-970f-290bf52c30e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fd6753e-971e-4b12-aabe-16d078990d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paths(models_list):\n",
    "    '''\n",
    "    Finds all the paths to forecasts and experiments metadata (directories /forecast/ and /wf_result/)\n",
    "    \n",
    "    Returns list with paths to forecast files, dict with metadata and list of all the experiment names\n",
    "    '''\n",
    "    \n",
    "    uuids = []\n",
    "    model_names = []\n",
    "    train_start_or_duration = []\n",
    "    hyperparameters = []\n",
    "    features = []\n",
    "    n_models = []\n",
    "    \n",
    "    paths_to_predictions = []\n",
    "    paths_to_info = []\n",
    "\n",
    "    forecast_paths = []\n",
    "    metadata_paths = []\n",
    "\n",
    "\n",
    "    for model in models_list:\n",
    "        paths_to_predictions += glob(f'/masters_diploma/forecast/{model}/research_task_*/{model}_*/')\n",
    "        paths_to_info += glob(f'/masters_diploma/wf_result/{model}/research_task_*')\n",
    "\n",
    "    print(len(paths_to_predictions))\n",
    "    print(len(paths_to_info))\n",
    "            \n",
    "    for path2 in paths_to_info:   \n",
    "#     for path2 in [max(paths_to_info, key=os.path.getctime)]:   # тільки для останнього експерименту\n",
    "        metadata_paths.extend(glob(os.path.join(path2, '*.csv')))\n",
    "\n",
    "\n",
    "    for path2 in paths_to_predictions:\n",
    "#     for path2 in [max(paths_to_predictions, key=os.path.getctime)]:   # тільки для останнього експерименту\n",
    "        prediction_paths = glob(os.path.join(path2, f'*.csv'))\n",
    "        if len(prediction_paths) > 0:\n",
    "            forecast_paths.append(prediction_paths)\n",
    "\n",
    "    yaml_file_paths = [f.replace('.csv', '.yaml') for f in metadata_paths]\n",
    "\n",
    "    metadata = {}\n",
    "    experiment_names = []\n",
    "    for file in yaml_file_paths: \n",
    "        with open(file, 'r') as f:\n",
    "            res = yaml.safe_load(f)\n",
    "\n",
    "        shorten_uuid = \"-\".join([res['unique_uuid'].split('-')[0], res['unique_uuid'].split('-')[-2]])\n",
    "        dur = res['duration_training_history'] if 'duration_training_history' in res else res['train_start']\n",
    "\n",
    "        metadata[shorten_uuid] = {\n",
    "            \"uuid\": res['unique_uuid'],\n",
    "            \"model_name\": res['model_name'],\n",
    "            \"duration/train_start\": dur,\n",
    "            \"hyperparameters\": res['model_hyperparameters'],\n",
    "            \"features\": res['train_features']\n",
    "        }\n",
    "\n",
    "        experiment_names.append(f\"{res['model_name']}_{shorten_uuid}\")\n",
    "\n",
    "\n",
    "    # for file in yaml_file_paths: \n",
    "    #     with open(file, 'r') as f:\n",
    "    #         res = yaml.safe_load(f)\n",
    "    #     uuids.append(res['unique_uuid'])\n",
    "    #     model_names.append(res['model_name'])\n",
    "    #     hyperparameters.append(res['model_hyperparameters'])\n",
    "    #     features.append(res['train_features'])\n",
    "    #     if 'duration_training_history' in res:\n",
    "    #         train_start_or_duration.append(res['duration_training_history'])\n",
    "    #     else:\n",
    "    #         train_start_or_duration.append(res['train_start'])\n",
    "     \n",
    "    # shorten_uuids = [\"-\".join([n.split('-')[0], n.split('-')[-2]]) for n in uuids]\n",
    "    # metadata_lst = list(zip(uuids, shorten_uuids, model_names, train_start_or_duration, hyperparameters, features, n_models))           \n",
    "    \n",
    "    # file_paths_splited = [metadata_paths[k].split('-') for k in range(len(metadata_paths))]\n",
    "    # model_name = [file_paths_splited[k][-5].split('\\\\')[1] for k in range(len(metadata_paths))]\n",
    "    # experiment_names = [\"-\".join([model_name[k], file_paths_splited[k][-2]]) for k in range(len(metadata_paths))]     \n",
    "    \n",
    "\n",
    "    return forecast_paths, metadata_paths, metadata, experiment_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b2dc372-5f6a-4b19-8699-e1fcac7205b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def facts(path_to_all):\n",
    "\n",
    "    dateparse = lambda dates: datetime.strptime(dates, '%Y-%m-%d %H:%M:%S')\n",
    "    path_to_weather = f'{path_to_all}/processed_data/history_weather.csv'\n",
    "\n",
    "    fact_temperature = pd.read_csv(\n",
    "        path_to_weather,\n",
    "        parse_dates=['date'],\n",
    "        index_col='date', \n",
    "        date_parser=dateparse\n",
    "    )[['temperature']]\n",
    "    \n",
    "    fact_temperature.index.name = 'date_time'\n",
    "\n",
    "    return fact_temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f16cdbd-7811-4c0a-b0b7-cc722f432396",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_forecasts_df(fact_pred, paths_to_exp_forecasts, exp_name):\n",
    "\n",
    "    '''\n",
    "    Creating a dataframe of forecasted temperature values\n",
    "    '''\n",
    "\n",
    "    dateparse = lambda dates: datetime.strptime(dates, '%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    df = fact_pred.copy()\n",
    "    \n",
    "    for num_exp, day_pred in enumerate(paths_to_exp_forecasts):\n",
    "        d = day_pred.split('_')[-4]\n",
    "        day_date = day_pred.split('\\\\')[-1].split('_')[-1].split(')')[0].split('(')[1]\n",
    "#         print(day_date)\n",
    "\n",
    "        pred = pd.read_csv(\n",
    "            day_pred,\n",
    "            parse_dates=['date_time'],\n",
    "            index_col='date_time', \n",
    "            date_parser=dateparse\n",
    "        )\n",
    "        \n",
    "        for h in range(24):\n",
    "            try:\n",
    "\n",
    "                df.loc[pd.to_datetime(day_date) + timedelta(hours=h), f'{exp_name}_{d}'] = pred.loc[pd.to_datetime(day_date) + timedelta(hours=h),'0']\n",
    "            \n",
    "            except KeyError as e:\n",
    "                \n",
    "                print(day_pred)\n",
    "                continue\n",
    "                \n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b68af895-e71f-4eb1-b253-5fe0a9f8558c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stat(fact_pred, info, day, path_to_files):\n",
    "    \n",
    "    forecast_cols = [col for col in fact_pred.columns if f'_d-{day}' in col]\n",
    "    df = fact_pred[['temperature'] + forecast_cols].dropna()\n",
    "    \n",
    "    absolute_errors = df[forecast_cols].sub(df['temperature'], axis=0)\n",
    "    \n",
    "    relative_errors = absolute_errors.div(df['temperature'], axis=0)\n",
    "    \n",
    "    print(absolute_errors.index)\n",
    "     \n",
    "    stat = pd.DataFrame({\n",
    "        'mean_abs_value': absolute_errors.abs().mean(),\n",
    "        'mean_rel_value': relative_errors.abs().mean(),\n",
    "        'median_abs_value': absolute_errors.abs().median(),\n",
    "        'median_rel_value': relative_errors.abs().median(),\n",
    "        'q25_abs_value': absolute_errors.abs().quantile(0.25),\n",
    "        'q25_rel_value': relative_errors.abs().quantile(0.25),\n",
    "        'q75_abs_value': absolute_errors.abs().quantile(0.75),\n",
    "        'q75_rel_value': relative_errors.abs().quantile(0.75)\n",
    "    })\n",
    "    \n",
    "    path = os.path.join(path_to_files, 'statistics', f'general_statistics_{day}.xlsx')\n",
    "    \n",
    "    if os.path.exists(path):\n",
    "    \n",
    "        gen_stat_df = pd.read_excel(path)\n",
    "        gen_stat_df = pd.concat([gen_stat_df, stat]).drop_duplicates()\n",
    "\n",
    "        gen_stat_df.to_excel(path, index=False)\n",
    "        \n",
    "    else:\n",
    "        stat.to_excel(path, index=False)    \n",
    "    \n",
    "    \n",
    "    stat_per_h = pd.DataFrame({\n",
    "        'mean_abs_value': absolute_errors.abs().groupby(df.index.hour).mean(),\n",
    "        'mean_rel_value': relative_errors.abs().groupby(df.index.hour).mean(),\n",
    "        'median_abs_value': absolute_errors.abs().groupby(df.index.hour).median(),\n",
    "        'median_rel_value': relative_errors.abs().groupby(df.index.hour).median(),\n",
    "        'q25_abs_value': absolute_errors.abs().groupby(df.index.hour).quantile(0.25),\n",
    "        'q25_rel_value': relative_errors.abs().groupby(df.index.hour).quantile(0.25),\n",
    "        'q75_abs_value': absolute_errors.abs().groupby(df.index.hour).quantile(0.75),\n",
    "        'q75_rel_value': relative_errors.abs().groupby(df.index.hour).quantile(0.75),\n",
    "    })\n",
    "    \n",
    "    return stat, stat_per_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a57871b-fc15-4ecd-9a6f-0e0c41ee595e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_models_per_hour(stat_per_h, day):\n",
    "    pass\n",
    "\n",
    "    return best_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f160e60c-345b-44f6-ab4b-984406581156",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "64089035-6622-47fa-8767-d6f85819ca59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164\n",
      "3\n",
      "DatetimeIndex(['2025-01-01 00:00:00', '2025-01-01 01:00:00',\n",
      "               '2025-01-01 02:00:00', '2025-01-01 03:00:00',\n",
      "               '2025-01-01 04:00:00', '2025-01-01 05:00:00',\n",
      "               '2025-01-01 06:00:00', '2025-01-01 07:00:00',\n",
      "               '2025-01-01 08:00:00', '2025-01-01 09:00:00',\n",
      "               ...\n",
      "               '2025-01-08 14:00:00', '2025-01-08 15:00:00',\n",
      "               '2025-01-08 16:00:00', '2025-01-08 17:00:00',\n",
      "               '2025-01-08 18:00:00', '2025-01-08 19:00:00',\n",
      "               '2025-01-08 20:00:00', '2025-01-08 21:00:00',\n",
      "               '2025-01-08 22:00:00', '2025-01-08 23:00:00'],\n",
      "              dtype='datetime64[ns]', name='date_time', length=192, freq=None)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "If using all scalar values, you must pass an index",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m fact_pred \u001b[38;5;241m=\u001b[39m fact_pred\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2025-01-01\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2025-01-08\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m4\u001b[39m):\n\u001b[1;32m---> 30\u001b[0m     stat, stat_per_h \u001b[38;5;241m=\u001b[39m \u001b[43mget_stat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfact_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath_to_all\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m     best_models_df \u001b[38;5;241m=\u001b[39m get_best_models_per_hour(stat_per_h, d)\n",
      "Cell \u001b[1;32mIn[14], line 36\u001b[0m, in \u001b[0;36mget_stat\u001b[1;34m(fact_pred, info, day, path_to_files)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     33\u001b[0m     stat\u001b[38;5;241m.\u001b[39mto_excel(path, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)    \n\u001b[1;32m---> 36\u001b[0m stat_per_h \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmean_abs_value\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mabsolute_errors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhour\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmean_rel_value\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrelative_errors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhour\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmedian_abs_value\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mabsolute_errors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhour\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmedian\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmedian_rel_value\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrelative_errors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhour\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmedian\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mq25_abs_value\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mabsolute_errors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhour\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.25\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mq25_rel_value\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrelative_errors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhour\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.25\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mq75_abs_value\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mabsolute_errors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhour\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.75\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mq75_rel_value\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrelative_errors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhour\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.75\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stat, stat_per_h\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:767\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    761\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[0;32m    762\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[0;32m    763\u001b[0m     )\n\u001b[0;32m    765\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    766\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 767\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[0;32m    769\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    499\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    500\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[0;32m    501\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[1;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:114\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 114\u001b[0m         index \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    116\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:667\u001b[0m, in \u001b[0;36m_extract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    664\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPer-column arrays must each be 1-dimensional\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m indexes \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m raw_lengths:\n\u001b[1;32m--> 667\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf using all scalar values, you must pass an index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    669\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m have_series:\n\u001b[0;32m    670\u001b[0m     index \u001b[38;5;241m=\u001b[39m union_indexes(indexes)\n",
      "\u001b[1;31mValueError\u001b[0m: If using all scalar values, you must pass an index"
     ]
    }
   ],
   "source": [
    "path_to_all = '/masters_diploma/'\n",
    "models_list = ['xgboost', 'random_forest', 'lightgbm']\n",
    "\n",
    "paths, metadata_paths, metadata_dict, exp_names = get_paths(models_list)\n",
    "\n",
    "fact_temperature = facts(path_to_all)\n",
    "fact_pred = fact_temperature.copy()\n",
    "\n",
    "\n",
    "for key, metadata in metadata_dict.items():\n",
    "#     print(key, metadata)\n",
    "    \n",
    "    exp_name = f\"{metadata['model_name']}_{key}\"\n",
    "    \n",
    "    for exp_forecasts in paths:\n",
    "        \n",
    "        k = exp_forecasts[0].split(\"\\\\\")[-2].split('-')\n",
    "        exp = \"-\".join([k[0], k[-2]])\n",
    "        \n",
    "        if exp == exp_name:\n",
    "\n",
    "            fact_pred = make_forecasts_df(fact_pred, exp_forecasts, exp)\n",
    "#             print(len(fact_pred.columns))\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "fact_pred = fact_pred.loc['2025-01-01':'2025-01-08']\n",
    "\n",
    "for d in range(4):\n",
    "    stat, stat_per_h = get_stat(fact_pred, metadata, d, path_to_all)\n",
    "    best_models_df = get_best_models_per_hour(stat_per_h, d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73889811-1303-413c-b4b7-b87f7f2f6d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
