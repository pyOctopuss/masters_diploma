{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c71220b6-2359-42b1-bc2f-83dd44049cf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "406b1fd0-b5b5-4083-af14-14f04d5192bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import re\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "import yaml\n",
    "from yaml import dump\n",
    "import uuid\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7437fae-d5e5-4c67-970f-290bf52c30e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fd6753e-971e-4b12-aabe-16d078990d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paths(models_list):\n",
    "    '''\n",
    "    Finds all the paths to forecasts and experiments metadata (directories /forecast/ and /wf_result/)\n",
    "    \n",
    "    Returns list with paths to forecast files, dict with metadata and list of all the experiment names\n",
    "    '''\n",
    "    \n",
    "    uuids = []\n",
    "    model_names = []\n",
    "    train_start_or_duration = []\n",
    "    hyperparameters = []\n",
    "    features = []\n",
    "    n_models = []\n",
    "    \n",
    "    paths_to_predictions = []\n",
    "    paths_to_info = []\n",
    "\n",
    "    forecast_paths = []\n",
    "    metadata_paths = []\n",
    "\n",
    "\n",
    "    for model in models_list:\n",
    "        paths_to_predictions += glob(f'/masters_diploma/forecast/{model}/research_task_*/{model}_*/')\n",
    "        paths_to_info += glob(f'/masters_diploma/wf_result/{model}/research_task_*')\n",
    "\n",
    "    print(len(paths_to_predictions))\n",
    "    print(len(paths_to_info))\n",
    "            \n",
    "    for path2 in paths_to_info:   \n",
    "#     for path2 in [max(paths_to_info, key=os.path.getctime)]:   # тільки для останнього експерименту\n",
    "        metadata_paths.extend(glob(os.path.join(path2, '*.csv')))\n",
    "\n",
    "\n",
    "    for path2 in paths_to_predictions:\n",
    "#     for path2 in [max(paths_to_predictions, key=os.path.getctime)]:   # тільки для останнього експерименту\n",
    "        prediction_paths = glob(os.path.join(path2, f'*.csv'))\n",
    "        if len(prediction_paths) > 0:\n",
    "            forecast_paths.append(prediction_paths)\n",
    "\n",
    "    yaml_file_paths = [f.replace('.csv', '.yaml') for f in metadata_paths]\n",
    "\n",
    "    metadata = {}\n",
    "    experiment_names = []\n",
    "    for file in yaml_file_paths: \n",
    "        with open(file, 'r') as f:\n",
    "            res = yaml.safe_load(f)\n",
    "\n",
    "        shorten_uuid = \"-\".join([res['unique_uuid'].split('-')[0], res['unique_uuid'].split('-')[-2]])\n",
    "        dur = res['duration_training_history'] if 'duration_training_history' in res else res['train_start']\n",
    "\n",
    "        metadata[shorten_uuid] = {\n",
    "            \"uuid\": res['unique_uuid'],\n",
    "            \"model_name\": res['model_name'],\n",
    "            \"duration/train_start\": dur,\n",
    "            \"hyperparameters\": res['model_hyperparameters'],\n",
    "            \"features\": res['train_features']\n",
    "        }\n",
    "\n",
    "        experiment_names.append(f\"{res['model_name']}-{shorten_uuid}\")\n",
    "\n",
    "\n",
    "    # for file in yaml_file_paths: \n",
    "    #     with open(file, 'r') as f:\n",
    "    #         res = yaml.safe_load(f)\n",
    "    #     uuids.append(res['unique_uuid'])\n",
    "    #     model_names.append(res['model_name'])\n",
    "    #     hyperparameters.append(res['model_hyperparameters'])\n",
    "    #     features.append(res['train_features'])\n",
    "    #     if 'duration_training_history' in res:\n",
    "    #         train_start_or_duration.append(res['duration_training_history'])\n",
    "    #     else:\n",
    "    #         train_start_or_duration.append(res['train_start'])\n",
    "     \n",
    "    # shorten_uuids = [\"-\".join([n.split('-')[0], n.split('-')[-2]]) for n in uuids]\n",
    "    # metadata_lst = list(zip(uuids, shorten_uuids, model_names, train_start_or_duration, hyperparameters, features, n_models))           \n",
    "    \n",
    "    # file_paths_splited = [metadata_paths[k].split('-') for k in range(len(metadata_paths))]\n",
    "    # model_name = [file_paths_splited[k][-5].split('\\\\')[1] for k in range(len(metadata_paths))]\n",
    "    # experiment_names = [\"-\".join([model_name[k], file_paths_splited[k][-2]]) for k in range(len(metadata_paths))]     \n",
    "    \n",
    "\n",
    "    return forecast_paths, metadata_paths, metadata, experiment_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f16cdbd-7811-4c0a-b0b7-cc722f432396",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_forecasts_df():\n",
    "    pass\n",
    "\n",
    "    return forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "318cd38f-2ac3-43bc-a3bc-4ab7313f0e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_fact(forecasts):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b68af895-e71f-4eb1-b253-5fe0a9f8558c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stat_for_d1():\n",
    "    pass\n",
    "\n",
    "    return stat_d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a57871b-fc15-4ecd-9a6f-0e0c41ee595e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_models_per_hour():\n",
    "    pass\n",
    "\n",
    "    return best_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f160e60c-345b-44f6-ab4b-984406581156",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64089035-6622-47fa-8767-d6f85819ca59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "path_to_files = '/masters_diploma/'\n",
    "models_list = ['xgboost', 'random_forest', 'lightgbm']\n",
    "\n",
    "paths, metadata_paths, metadata_dict, exp_names = get_paths(models_list)\n",
    "\n",
    "\n",
    "for key, metadata in metadata_dict.items():\n",
    "    \n",
    "    exp_name = f\"{info['model_name']}_{key}\"\n",
    "    \n",
    "    for exp_forecasts in paths:\n",
    "        \n",
    "        k = exp_forecasts[0].split(\"\\\\\")[-2].split('-')\n",
    "        exp = \"-\".join([k[0], k[-2]])\n",
    "        \n",
    "        if exp == exp_name:\n",
    "\n",
    "            df_preds = make_forecasts_df(exp_forecasts, metadata, exp_name)\n",
    "            stat, stat_per_h = get_stat_for_d1(df_preds, df_preds.shape[0], exp_name)\n",
    "            best_models_df = get_best_models_per_hour(stat_per_h)\n",
    "            \n",
    "        else:\n",
    "            pass\n",
    "\n",
    "\n",
    "gen_stat_df = df.read_csv(os.path.join(path_to_files, 'statistics', 'general_statistics'))\n",
    "gen_stat_df = pd.concat([gen_stat_df, stat]).drop_duplicates()\n",
    "\n",
    "gen_stat_df.to_csv(os.path.join(path_to_files, 'statistics', 'general_statistics'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546e481c-1c30-4ca5-8fb5-f6a862f76601",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6b3f43-c45a-4fea-8633-de92039b4046",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133e90b2-53b9-43e7-b174-d2c87f91c4ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73889811-1303-413c-b4b7-b87f7f2f6d3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
