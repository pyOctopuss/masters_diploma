{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c71220b6-2359-42b1-bc2f-83dd44049cf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "406b1fd0-b5b5-4083-af14-14f04d5192bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import re\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "import yaml\n",
    "from yaml import dump\n",
    "import uuid\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7437fae-d5e5-4c67-970f-290bf52c30e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fd6753e-971e-4b12-aabe-16d078990d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paths(models_list):\n",
    "    '''\n",
    "    Finds all the paths to forecasts and experiments metadata (directories /forecast/ and /wf_result/)\n",
    "    \n",
    "    Returns list with paths to forecast files, dict with metadata and list of all the experiment names\n",
    "    '''\n",
    "    \n",
    "    uuids = []\n",
    "    model_names = []\n",
    "    train_start_or_duration = []\n",
    "    hyperparameters = []\n",
    "    features = []\n",
    "    n_models = []\n",
    "    \n",
    "    paths_to_predictions = []\n",
    "    paths_to_info = []\n",
    "\n",
    "    forecast_paths = []\n",
    "    metadata_paths = []\n",
    "\n",
    "\n",
    "    for model in models_list:\n",
    "        paths_to_predictions += glob(f'/masters_diploma/forecast/{model}/research_task_*/{model}_*/')\n",
    "        paths_to_info += glob(f'/masters_diploma/wf_result/{model}/research_task_*')\n",
    "\n",
    "#     print(len(paths_to_predictions))\n",
    "#     print(len(paths_to_info))\n",
    "            \n",
    "    for path2 in paths_to_info:   \n",
    "#     for path2 in [max(paths_to_info, key=os.path.getctime)]:   # тільки для останнього експерименту\n",
    "        metadata_paths.extend(glob(os.path.join(path2, '*.csv')))\n",
    "\n",
    "\n",
    "    for path2 in paths_to_predictions:\n",
    "#     for path2 in [max(paths_to_predictions, key=os.path.getctime)]:   # тільки для останнього експерименту\n",
    "        prediction_paths = glob(os.path.join(path2, f'*.csv'))\n",
    "        if len(prediction_paths) > 0:\n",
    "            forecast_paths.append(prediction_paths)\n",
    "\n",
    "    yaml_file_paths = [f.replace('.csv', '.yaml') for f in metadata_paths]\n",
    "\n",
    "    metadata = {}\n",
    "    experiment_names = []\n",
    "    for file in yaml_file_paths: \n",
    "        with open(file, 'r') as f:\n",
    "            res = yaml.safe_load(f)\n",
    "\n",
    "        shorten_uuid = \"-\".join([res['unique_uuid'].split('-')[0], res['unique_uuid'].split('-')[-2]])\n",
    "        dur = res['duration_training_history'] if 'duration_training_history' in res else res['train_start']\n",
    "\n",
    "        metadata[shorten_uuid] = {\n",
    "            \"uuid\": res['unique_uuid'],\n",
    "            \"model_name\": res['model_name'],\n",
    "            \"duration/train_start\": dur,\n",
    "            \"hyperparameters\": res['model_hyperparameters'],\n",
    "            \"features\": res['train_features']\n",
    "        }\n",
    "\n",
    "        experiment_names.append(f\"{res['model_name']}_{shorten_uuid}\")   \n",
    "    \n",
    "\n",
    "    return forecast_paths, metadata_paths, metadata, experiment_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b2dc372-5f6a-4b19-8699-e1fcac7205b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def facts(path_to_all):\n",
    "\n",
    "    dateparse = lambda dates: datetime.strptime(dates, '%Y-%m-%d %H:%M:%S')\n",
    "    path_to_weather = f'{path_to_all}/processed_data/history_weather.csv'\n",
    "\n",
    "    fact_temperature = pd.read_csv(\n",
    "        path_to_weather,\n",
    "        parse_dates=['date'],\n",
    "        index_col='date', \n",
    "        date_parser=dateparse\n",
    "    )[['temperature']]\n",
    "    \n",
    "    fact_temperature.index.name = 'date_time'\n",
    "\n",
    "    return fact_temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f16cdbd-7811-4c0a-b0b7-cc722f432396",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_forecasts_df(fact_pred, paths_to_exp_forecasts, exp_name):\n",
    "\n",
    "    '''\n",
    "    Creating a dataframe of forecasted temperature values\n",
    "    '''\n",
    "\n",
    "    dateparse = lambda dates: datetime.strptime(dates, '%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    df = fact_pred.copy()\n",
    "    \n",
    "    for num_exp, day_pred in enumerate(paths_to_exp_forecasts):\n",
    "        _ = day_pred.split('_')\n",
    "        if \"_\".join([_[-4], _[-3]]) == 'random_forest':\n",
    "            d = _[-5]\n",
    "        else:\n",
    "            d = _[-4]\n",
    "            \n",
    "        day_date = day_pred.split('\\\\')[-1].split('_')[-1].split(')')[0].split('(')[1]\n",
    "#         print(day_date)\n",
    "\n",
    "        pred = pd.read_csv(\n",
    "            day_pred,\n",
    "            parse_dates=['date_time'],\n",
    "            index_col='date_time', \n",
    "            date_parser=dateparse\n",
    "        )\n",
    "        \n",
    "        for h in range(24):\n",
    "            try:\n",
    "\n",
    "                df.loc[pd.to_datetime(day_date) + timedelta(hours=h), f'{exp_name}_{d}'] = pred.loc[pd.to_datetime(day_date) + timedelta(hours=h),'0']\n",
    "            \n",
    "            except KeyError as e:\n",
    "                \n",
    "                print(day_pred)\n",
    "                continue\n",
    "                \n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b68af895-e71f-4eb1-b253-5fe0a9f8558c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stat(fact_pred, info, day, path_to_files):\n",
    "    \n",
    "    forecast_cols = [col for col in fact_pred.columns if day in col]\n",
    "    df = fact_pred[['temperature'] + forecast_cols].dropna()\n",
    "    \n",
    "    df.columns = df.columns.str.replace(r'_d-\\d+$', '', regex=True)\n",
    "\n",
    "    \n",
    "    absolute_errors = df[df.columns[1:]].sub(df['temperature'], axis=0)\n",
    "    \n",
    "    relative_errors = absolute_errors.div(df['temperature'], axis=0)\n",
    "    \n",
    "#     print(absolute_errors.columns)\n",
    "    \n",
    "    stat_dict = {}\n",
    "    for exp in info.keys():\n",
    "        exp_name = f\"{info[exp]['model_name']}_{exp}\"\n",
    "        \n",
    "        stat_dict[exp] = {\n",
    "            'mean_abs_value': absolute_errors[exp_name].abs().mean(),\n",
    "            'mean_rel_value': relative_errors[exp_name].abs().mean(),\n",
    "            'median_abs_value': absolute_errors[exp_name].abs().median(),\n",
    "            'median_rel_value': relative_errors[exp_name].abs().median(),\n",
    "            'q25_abs_value': absolute_errors[exp_name].abs().quantile(0.25),\n",
    "            'q25_rel_value': relative_errors[exp_name].abs().quantile(0.25),\n",
    "            'q75_abs_value': absolute_errors[exp_name].abs().quantile(0.75),\n",
    "            'q75_rel_value': relative_errors[exp_name].abs().quantile(0.75),\n",
    "            \"model_name\": info[exp][\"model_name\"],\n",
    "            \"hyperparameters\": f'{info[exp][\"hyperparameters\"]}',\n",
    "            \"features\": f'{info[exp][\"features\"]}',\n",
    "            \"train_start\": info[exp][\"duration/train_start\"]\n",
    "        }\n",
    "        \n",
    "    stat = pd.DataFrame(stat_dict).T\n",
    "        \n",
    "    stat_per_h = pd.DataFrame(relative_errors.abs().groupby(df.index.hour).median(), columns=relative_errors.columns)\n",
    "    \n",
    "    \n",
    "    path = os.path.join(path_to_files, 'statistics', f'general_statistics_{day}.xlsx')\n",
    "    path_h = os.path.join(path_to_files, 'statistics', f'general_statistics_{day}_by_hour.xlsx')\n",
    "    \n",
    "    if os.path.exists(path):\n",
    "    \n",
    "        gen_stat_df = pd.read_excel(path)\n",
    "        gen_stat_h_df = pd.read_excel(path_h)\n",
    "        gen_stat_df = pd.concat([gen_stat_df, stat]).drop_duplicates()\n",
    "        gen_stat_h_df = pd.concat([gen_stat_h_df, stat_per_h]).drop_duplicates()\n",
    "\n",
    "        gen_stat_df.to_excel(path, index=False)\n",
    "        gen_stat_h_df.to_excel(path_h, index=False)\n",
    "        \n",
    "    else:\n",
    "        stat.to_excel(path, index=False)\n",
    "        stat_per_h.to_excel(path_h)\n",
    "    \n",
    "    return stat, stat_per_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a57871b-fc15-4ecd-9a6f-0e0c41ee595e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_models_per_hour(stat_per_h, day, info, path_to_files):\n",
    "    \n",
    "    min_errors = stat_per_h.min(axis=1)\n",
    "    best_exps = stat_per_h.idxmin(axis=1)\n",
    "    \n",
    "    \n",
    "    best_models = pd.concat([best_exps, min_errors], axis=1)\n",
    "    best_models.columns=['experiment_name', 'median_rel_err_value']\n",
    "    \n",
    "    for h in best_models.index:\n",
    "        exp = best_models.loc[h, 'experiment_name']\n",
    "        meta = info[exp.split('_')[-1]]\n",
    "        \n",
    "        best_models.loc[best_models['experiment_name']==exp, 'model_name'] = meta['model_name']\n",
    "        best_models.loc[best_models['experiment_name']==exp, 'hyperparameters'] = f\"{meta['hyperparameters']}\"\n",
    "        best_models.loc[best_models['experiment_name']==exp, 'features'] = f\"{meta['features']}\"\n",
    "        best_models.loc[best_models['experiment_name']==exp, 'duration/train_start'] = meta['duration/train_start']\n",
    "        \n",
    "        \n",
    "    path = get_next_versioned_filename(os.path.join(path_to_files, 'statistics', 'best_for_hour'), day)\n",
    "    best_models.to_excel(path)\n",
    "    \n",
    "    \n",
    "    return best_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f160e60c-345b-44f6-ab4b-984406581156",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_versioned_filename(base_dir, day, prefix=\"hourly_best\", ext=\".xlsx\"):\n",
    "    today = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "    pattern = re.compile(rf\"{prefix}_{day}_{today}_v(\\d+){re.escape(ext)}\")\n",
    "    \n",
    "    # Отримаємо всі файли в директорії, які відповідають шаблону\n",
    "    existing_versions = []\n",
    "    for filename in os.listdir(base_dir):\n",
    "        match = pattern.match(filename)\n",
    "        if match:\n",
    "            existing_versions.append(int(match.group(1)))\n",
    "    \n",
    "    next_version = max(existing_versions, default=0) + 1\n",
    "    new_filename = f\"{prefix}_{day}_{today}_v{next_version}{ext}\"\n",
    "    \n",
    "    \n",
    "    return os.path.join(base_dir, new_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64089035-6622-47fa-8767-d6f85819ca59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gathering experiment info...\n",
      "loading fact temperature dataset...\n",
      "adding experiments` forecasts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 164/164 [02:35<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "calculating statistics for day 0...\n",
      "finished\n",
      "finding best model for hour...\n",
      "done\n",
      "\n",
      "\n",
      "calculating statistics for day 1...\n",
      "finished\n",
      "finding best model for hour...\n",
      "done\n",
      "\n",
      "\n",
      "calculating statistics for day 2...\n",
      "finished\n",
      "finding best model for hour...\n",
      "done\n",
      "\n",
      "\n",
      "calculating statistics for day 3...\n",
      "finished\n",
      "finding best model for hour...\n",
      "done\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path_to_all = '/masters_diploma/'\n",
    "models_list = ['xgboost', 'random_forest', 'lightgbm']\n",
    "\n",
    "print('gathering experiment info...')\n",
    "paths, metadata_paths, metadata_dict, exp_names = get_paths(models_list)\n",
    "\n",
    "print('loading fact temperature dataset...')\n",
    "fact_temperature = facts(path_to_all)\n",
    "fact_pred = fact_temperature.copy()\n",
    "\n",
    "print('adding experiments` forecasts...')\n",
    "\n",
    "\n",
    "for exp_forecasts in tqdm(paths):\n",
    "      \n",
    "    k = exp_forecasts[0].split(\"\\\\\")[-2].split('-')\n",
    "    exp = \"-\".join([k[0], k[-2]])\n",
    "    \n",
    "    fact_pred = make_forecasts_df(fact_pred, exp_forecasts, exp)\n",
    "            \n",
    "            \n",
    "# for key, metadata in tqdm(metadata_dict.items()):\n",
    "# #     print(key, metadata)\n",
    "    \n",
    "#     exp_name = f\"{metadata['model_name']}_{key}\"\n",
    "    \n",
    "#     for exp_forecasts in paths:\n",
    "        \n",
    "#         k = exp_forecasts[0].split(\"\\\\\")[-2].split('-')\n",
    "#         exp = \"-\".join([k[0], k[-2]])\n",
    "        \n",
    "#         if exp == exp_name:\n",
    "\n",
    "#             fact_pred = make_forecasts_df(fact_pred, exp_forecasts, exp)\n",
    "# #             print(len(fact_pred.columns))\n",
    "#         else:\n",
    "#             continue\n",
    "\n",
    "fact_pred = fact_pred.loc['2025-01-01':'2025-01-08']\n",
    "\n",
    "for d in range(4):\n",
    "    print(f'\\ncalculating statistics for day {d}...')\n",
    "    stat, stat_per_h = get_stat(fact_pred, metadata_dict, f'd-{d}', path_to_all)\n",
    "    print('finished')\n",
    "    print('finding best model for hour...')\n",
    "    best_models_df = get_best_models_per_hour(stat_per_h, f'd-{d}', metadata_dict, path_to_all)\n",
    "    print('done\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
